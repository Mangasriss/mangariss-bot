import logging
import json
import os
import sys  # <--- Ajoute Ã§a
import scraper
import storage

# --- CONFIG LOGS ---
if not os.path.exists('logs'): os.makedirs('logs')

# Configuration spÃ©ciale pour Windows (Support des emojis)
# On redirige la sortie standard vers l'UTF-8
sys.stdout.reconfigure(encoding='utf-8')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("logs/bot.log", encoding='utf-8'), # UTF-8 pour le fichier
        logging.StreamHandler(sys.stdout) # UTF-8 forcÃ© pour l'Ã©cran
    ]
)
logger = logging.getLogger()

# --- CONFIG B2 CLUSTER ---
# âš ï¸ METS ICI TON CLUSTER B2 EXACT (f002, f004, s001...)
B2_CLUSTER = "f003" 

def load_config():
    with open('config.json', 'r') as f: return json.load(f)
def load_mangas():
    if not os.path.exists('mangas.txt'): return []
    with open('mangas.txt', 'r') as f: return [line.strip() for line in f if line.strip()]

def main():
    logger.info("ðŸ¤– --- DÃ‰MARRAGE BOT API ---")
    config = load_config()
    tracked_names = load_mangas()
    bot_scraper = scraper.MangaScraper(config)
    BUCKET = storage.creds['bucket_name']

    # 1. Gestion des Covers
    logger.info("ðŸ–¼ï¸ VÃ©rification des covers...")
    covers_url_map = {}
    for m in tracked_names:
        url = storage.upload_cover(m)
        # Si pas de cover uploadÃ©e, on gÃ©nÃ¨re l'URL supposÃ©e
        if url: 
            covers_url_map[m] = url
        else:
            # URL si la cover existe dÃ©jÃ  sur le cloud
            covers_url_map[m] = f"https://{B2_CLUSTER}.backblazeb2.com/file/{BUCKET}/mangas/{m}/cover.jpg"

    # 2. Scan Site Source
    logger.info("ðŸ“¡ Scan des nouveautÃ©s...")
    found_chapters = bot_scraper.get_latest_chapters_from_feed(config['pages_to_scan'], tracked_names)

    # 3. Base de donnÃ©es en mÃ©moire
    # On va construire la structure de notre API
    db_store = {} 

    # On initialise la DB avec les mangas trackÃ©s
    for m in tracked_names:
        db_store[m] = {
            "title": m,
            "author": "Inconnu", # Sera mis Ã  jour si trouvÃ© dans le feed
            "cover": covers_url_map.get(m, ""),
            "chapters": []
        }

    # 4. Traitement des chapitres
    for chap in found_chapters:
        m_name = chap['manga_name']
        c_num = chap['chapter_num']
        
        # Mise Ã  jour auteur
        if chap['author'] != "Inconnu":
            db_store[m_name]['author'] = chap['author']

        logger.info(f"ðŸ”Ž Analyse : {m_name} {c_num}")

        # 1. On rÃ©cupÃ¨re la liste des fichiers DÃ‰JÃ€ sur B2 pour ce chapitre
        # Cela nous permet de savoir si le chapitre est complet ou partiel
        existing_files = storage.list_files_in_chapter(m_name, c_num)
        
        # URL du dossier B2 (pour le JSON plus tard)
        folder_url = f"https://{B2_CLUSTER}.backblazeb2.com/file/{BUCKET}/mangas/{m_name}/{c_num}/"
        
        # Compteur de pages (On part de ce qu'on a dÃ©jÃ )
        total_pages_count = len(existing_files)

        # 2. On lance le scraper en mode "Flux Tendu"
        # Le scraper va tester 01.png, 02.png...
        for filename, content in bot_scraper.download_images_generator(chap['scan_id']):
            
            # VÃ‰RIFICATION : Est-ce qu'on a dÃ©jÃ  cette image ?
            if filename in existing_files:
                # Si oui, on ne l'upload pas, mais on logue pour rassurer (optionnel, on peut commenter pour allÃ©ger)
                # logger.info(f"      â© IgnorÃ© (DÃ©jÃ  prÃ©sent) : {filename}")
                pass
            else:
                # Si non, on l'upload IMMÃ‰DIATEMENT
                size_ko = len(content) / 1024
                storage.upload_image(m_name, c_num, filename, content)
                logger.info(f"      â˜ï¸ UPLOADÃ‰ : {filename} ({size_ko:.1f} Ko)")
                
                # On l'ajoute Ã  notre liste locale pour que le compte soit bon
                existing_files.add(filename)

        # Ã€ la fin de la boucle, le chapitre est forcÃ©ment complet (ou au max possible)
        total_pages_count = len(existing_files)
        logger.info(f"   âœ… Chapitre traitÃ© ({total_pages_count} pages au total)")

        # Ajout au JSON
        db_store[m_name]['chapters'].append({
            "number": c_num,
            "title": chap['chapter_title'],
            "folder_url": folder_url,
            "pages_count": total_pages_count
        })

    # 5. GÃ‰NÃ‰RATION DES FICHIERS API (JSON)
    if not os.path.exists('api/details'): os.makedirs('api/details')

    # A. mangas.json (Liste globale)
    api_list = []
    for m_name, data in db_store.items():
        slug = m_name.lower().replace(' ', '-')
        api_list.append({
            "id": slug,
            "title": m_name,
            "cover": data['cover']
        })
    
    with open('api/mangas.json', 'w', encoding='utf-8') as f:
        json.dump(api_list, f, indent=2, ensure_ascii=False)

    # B. details/{slug}.json (DÃ©tail par manga)
    for m_name, data in db_store.items():
        slug = m_name.lower().replace(' ', '-')
        with open(f'api/details/{slug}.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

    logger.info("âœ… API JSON gÃ©nÃ©rÃ©e avec succÃ¨s !")

if __name__ == "__main__":
    main()